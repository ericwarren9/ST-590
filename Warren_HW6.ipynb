{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1919615b-dd5c-4caa-b627-98694f3e3171",
   "metadata": {},
   "source": [
    "# ST 590 Homework 6 By: Eric Warren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac46464-aa0c-4604-bf6f-ae565ebc048e",
   "metadata": {},
   "source": [
    "## Part 1: Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b2f10-344c-40d9-99db-c9774d727f35",
   "metadata": {},
   "source": [
    "Using the NFL box score data set ([https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv](https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv)), we are going to split the data into separate .csv files based on the season. That is, you want to subset the data to obtain just one season and output that to a .csv file. You want to do this process for each season in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0afa67f-bd3f-4b94-9b48-f59a4abdacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the csv file\n",
    "nfl_full = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\n",
    "\n",
    "# Split the data into new csvs\n",
    "for i, value in enumerate(nfl_full['season']):\n",
    "        nfl_full[nfl_full['season'] == value].to_csv(r'NFL_season_'+str(value)+r'.csv',index = False, na_rep = 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ce143-ea11-4fff-a600-5b498efd3fd8",
   "metadata": {},
   "source": [
    "## Part 2: MapReduce Idea (no pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc33e3-161c-4c6e-8688-d7c15f11f2b4",
   "metadata": {},
   "source": [
    "### MapReduce Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78275103-d5f5-4fca-a065-8d6fee9fe26b",
   "metadata": {},
   "source": [
    "- Consider a variable to group on (in our case we are going to use `week`) and a numeric target variable (in our case we are going to use `AQ1`)\n",
    "- Now we are going to write a mapping function to find the\n",
    "  - Sum of the target variable across the grouping variable\n",
    "  - Sum of the squared values of the target variable across the grouping variable\n",
    "  - Count or number of observations of the target variable in each group\n",
    "- Lastly, write a reduce function to combine the results across the season data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18d7234d-3b6e-4fe0-a61c-9d48906c33f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>sum</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>size</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>695</td>\n",
       "      <td>16</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>686</td>\n",
       "      <td>14</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>450</td>\n",
       "      <td>16</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>666</td>\n",
       "      <td>16</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>534</td>\n",
       "      <td>16</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>192</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ConfChamp</td>\n",
       "      <td>13</td>\n",
       "      <td>169</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Division</td>\n",
       "      <td>21</td>\n",
       "      <td>245</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WildCard</td>\n",
       "      <td>21</td>\n",
       "      <td>245</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         week  sum  sum_sq  size season\n",
       "0           1   81     695    16   2002\n",
       "1          10   78     686    14   2002\n",
       "2          11   52     450    16   2002\n",
       "3          12   68     666    16   2002\n",
       "4          13   56     534    16   2002\n",
       "..        ...  ...     ...   ...    ...\n",
       "16          9   36     192    13   2014\n",
       "17  ConfChamp   13     169     2   2014\n",
       "18   Division   21     245     4   2014\n",
       "19  SuperBowl    0       0     1   2014\n",
       "20   WildCard   21     245     4   2014\n",
       "\n",
       "[273 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import reduce function\n",
    "from functools import reduce\n",
    "\n",
    "# Create mapping function\n",
    "def find_sums(file, grouping_var = \"week\", numeric_var = \"AQ1\"):\n",
    "  \"\"\"\n",
    "  We are first going to read in our data based on the file given\n",
    "  In this function we are going to find the sum of the target variable across the grouping variable\n",
    "  We are also going to find the sum of squared values across the grouping variable\n",
    "  Lastly, we should return the count of the variable for each of its groups\n",
    "  \"\"\"\n",
    "\n",
    "  # Read in the data\n",
    "  data = pd.read_csv(file)\n",
    "\n",
    "  # Get the sum of a value\n",
    "  sum = data.groupby(by = [grouping_var], as_index = False)[numeric_var].sum()\n",
    "\n",
    "  # Get the sum of squared values\n",
    "  sum_sq = data[numeric_var].pow(2).groupby(data[grouping_var]).sum()\n",
    "\n",
    "  # Get the count present\n",
    "  count = data.groupby(by = [grouping_var], as_index = False).size()\n",
    "\n",
    "  # Combine the data together\n",
    "  values = reduce(lambda x, y: pd.merge(x, y, on = grouping_var), [sum, sum_sq, count])\n",
    "\n",
    "  # Get the season\n",
    "  values['season'] = file[-8:-4]\n",
    "\n",
    "  # Rename some columns to make it easier to follow\n",
    "  values = values.rename(columns={numeric_var + \"_x\" : \"sum\", numeric_var + \"_y\": \"sum_sq\"})\n",
    "  return values\n",
    "\n",
    "# Now create list to get the different csv files\n",
    "lst_seasons = []\n",
    "for szn in range(nfl_full.season.min(), nfl_full.season.max() + 1):\n",
    "  lst_seasons.append(\"NFL_season_\" + str(szn) + \".csv\")\n",
    "\n",
    "# Now get the values for each season to show that this can work\n",
    "results = list(map(lambda x: find_sums(x), lst_seasons)) # This gives list of dfs\n",
    "results_df = pd.concat(results) # This combines list of dfs into a larger df\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb6af0-0639-45ac-8008-34c3d1a2cf9f",
   "metadata": {},
   "source": [
    "### Summarizing Bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ddeaf-98a2-4cea-a57c-05ee867e0f60",
   "metadata": {},
   "source": [
    "- Now take the final result and use it to construct the\n",
    "  - mean at each level of the grouping variable\n",
    "  - standard deviation at each level of the grouping variable (if the count for the group was larger than 1)\n",
    "- We are going to show our function works for the 2002 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40705fd3-bb72-414d-b287-c19a330c19e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>4.358421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>4.397802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>4.328202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.746929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.316641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.930649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>5.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>4.134711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>5.256345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.676477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>3.024624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>3.997252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.24771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>2.590133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>4.200994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>4.811273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ConfChamp</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>2.12132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Division</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WildCard</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.715476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         week      mean        sd\n",
       "0           1  5.062500  4.358421\n",
       "1          10  5.571429  4.397802\n",
       "2          11  3.250000  4.328202\n",
       "3          12  4.250000  5.013316\n",
       "4          13  3.500000  4.746929\n",
       "5          14  5.500000  5.316641\n",
       "6          15  3.125000  3.930649\n",
       "7          16  5.750000  5.013316\n",
       "8          17  3.187500  4.134711\n",
       "9           2  4.187500  5.256345\n",
       "10          3  3.142857  3.676477\n",
       "11          4  4.928571  3.024624\n",
       "12          5  2.142857  3.997252\n",
       "13          6  5.000000   5.24771\n",
       "14          7  1.642857  2.590133\n",
       "15          8  2.428571  4.200994\n",
       "16          9  5.071429  4.811273\n",
       "17  ConfChamp  8.500000   2.12132\n",
       "18   Division  1.500000  1.732051\n",
       "19  SuperBowl  3.000000      None\n",
       "20   WildCard  7.000000  5.715476"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import numpy to use where function\n",
    "import numpy as np\n",
    "\n",
    "# Create a function to find the mean and sd\n",
    "def find_mean_sd(data, grouping_var = \"week\"):\n",
    "  \"\"\"\n",
    "  Here we would like to find the mean and standard deviation at each level of grouping variable\n",
    "  \"\"\"\n",
    "  # First get a sum of the sum of data for numeric column\n",
    "  sum = data.groupby(by = [grouping_var], as_index = False)['sum'].sum()\n",
    "\n",
    "  # Now get a sum of the sum of squares of data for numeric column\n",
    "  sum_sq = data.groupby(by = [grouping_var], as_index = False)['sum_sq'].sum()\n",
    "\n",
    "  # Now get a sum of the count of data for numeric column\n",
    "  size = data.groupby(by = [grouping_var], as_index = False)['size'].sum()\n",
    "\n",
    "  # Now combine into one file\n",
    "  df = reduce(lambda x, y: pd.merge(x, y, on = grouping_var), [sum, sum_sq, size])\n",
    "\n",
    "  # Get the mean by group\n",
    "  df['mean'] = df['sum'] / df['size']\n",
    "\n",
    "  # Now get the standard deviation\n",
    "  df['sd'] = np.where(df['size'] == 1, \n",
    "                      None, \n",
    "                      np.sqrt((1 / (df['size'] - 1)) * (df['sum_sq'] - (df['mean'] ** 2 * df['size']))))\n",
    "\n",
    "  return df[['week', 'mean', 'sd']]\n",
    "\n",
    "# Show this works on data\n",
    "test = find_sums(\"NFL_season_2002.csv\") # 2002 season\n",
    "find_mean_sd(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8639f-9921-4000-9ef1-3a379279ed3b",
   "metadata": {},
   "source": [
    "Now that we can see our mean and standard deviation function works for our 2002 NFL season, we can do this to find the values for all the seasons grouped together.\n",
    "\n",
    "Lastly, we are going to create a function to put the MapReduce part and the final calculation part into an easy to use function, allowing you to change the grouping variable and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7a2f979-ea4b-45a0-a2b4-8168ca45c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_mean_sd(file, grouping_var = \"week\", numeric_var = \"AQ1\"):\n",
    "  \"\"\"\n",
    "  Here we are going to combine our function to get sum, sum squares, and length\n",
    "  into our other function to get sample mean and sample standard deviations\n",
    "  from our files directly\n",
    "  \"\"\"\n",
    "  \n",
    "  # First get the data we need from the first function\n",
    "  if isinstance(file, str):\n",
    "    data = find_sums(file, grouping_var = grouping_var, numeric_var = numeric_var)\n",
    "  else:\n",
    "    results = list(map(lambda x: find_sums(x), file)) # This gives list of dfs\n",
    "    data = pd.concat(results)\n",
    "\n",
    "  # Now use other function to get the mean and standard deviation of each grouping\n",
    "  final_data = find_mean_sd(data, grouping_var = grouping_var)\n",
    "\n",
    "  # Return our final groupings \n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20cc54-7d0b-408f-94ac-f96e8a52c3f3",
   "metadata": {},
   "source": [
    "We can check this works on a simple data set being our 2002 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58fc543a-5e70-4efd-a1cb-5669bf0bc831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>4.358421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>4.397802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>4.328202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.746929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.316641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.930649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>5.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>4.134711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>5.256345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.676477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>3.024624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>3.997252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.24771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>2.590133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>4.200994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>4.811273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ConfChamp</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>2.12132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Division</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WildCard</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.715476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         week      mean        sd\n",
       "0           1  5.062500  4.358421\n",
       "1          10  5.571429  4.397802\n",
       "2          11  3.250000  4.328202\n",
       "3          12  4.250000  5.013316\n",
       "4          13  3.500000  4.746929\n",
       "5          14  5.500000  5.316641\n",
       "6          15  3.125000  3.930649\n",
       "7          16  5.750000  5.013316\n",
       "8          17  3.187500  4.134711\n",
       "9           2  4.187500  5.256345\n",
       "10          3  3.142857  3.676477\n",
       "11          4  4.928571  3.024624\n",
       "12          5  2.142857  3.997252\n",
       "13          6  5.000000   5.24771\n",
       "14          7  1.642857  2.590133\n",
       "15          8  2.428571  4.200994\n",
       "16          9  5.071429  4.811273\n",
       "17  ConfChamp  8.500000   2.12132\n",
       "18   Division  1.500000  1.732051\n",
       "19  SuperBowl  3.000000      None\n",
       "20   WildCard  7.000000  5.715476"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show it works for our 2002 data\n",
    "grouping_mean_sd(\"NFL_season_2002.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66aac57-8048-4b81-afe1-075842120891",
   "metadata": {},
   "source": [
    "Now show it works for all seasons we are trying to look at (2002 - 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9aea3a6-3a80-40d5-bdf3-41eb9bdf4a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.427885</td>\n",
       "      <td>4.041403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4.580645</td>\n",
       "      <td>4.45111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>3.970149</td>\n",
       "      <td>4.513214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>3.653659</td>\n",
       "      <td>4.311641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>4.192308</td>\n",
       "      <td>4.765165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>3.836538</td>\n",
       "      <td>4.502518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>3.913462</td>\n",
       "      <td>4.63261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>4.096154</td>\n",
       "      <td>4.239267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>3.725962</td>\n",
       "      <td>4.548918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3.531401</td>\n",
       "      <td>4.195204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>4.202020</td>\n",
       "      <td>4.467548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3.967391</td>\n",
       "      <td>4.283533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>3.565934</td>\n",
       "      <td>4.431622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>4.143646</td>\n",
       "      <td>4.629774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>4.311111</td>\n",
       "      <td>4.733441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>3.342697</td>\n",
       "      <td>3.924496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>4.179775</td>\n",
       "      <td>5.218528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ConfChamp</td>\n",
       "      <td>3.192308</td>\n",
       "      <td>4.436388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Division</td>\n",
       "      <td>4.673077</td>\n",
       "      <td>5.227083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SuperBowl</td>\n",
       "      <td>2.923077</td>\n",
       "      <td>3.09466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WildCard</td>\n",
       "      <td>4.403846</td>\n",
       "      <td>5.262742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         week      mean        sd\n",
       "0           1  3.427885  4.041403\n",
       "1          10  4.580645   4.45111\n",
       "2          11  3.970149  4.513214\n",
       "3          12  3.653659  4.311641\n",
       "4          13  4.192308  4.765165\n",
       "5          14  3.836538  4.502518\n",
       "6          15  3.913462   4.63261\n",
       "7          16  4.096154  4.239267\n",
       "8          17  3.725962  4.548918\n",
       "9           2  3.531401  4.195204\n",
       "10          3  4.202020  4.467548\n",
       "11          4  3.967391  4.283533\n",
       "12          5  3.565934  4.431622\n",
       "13          6  4.143646  4.629774\n",
       "14          7  4.311111  4.733441\n",
       "15          8  3.342697  3.924496\n",
       "16          9  4.179775  5.218528\n",
       "17  ConfChamp  3.192308  4.436388\n",
       "18   Division  4.673077  5.227083\n",
       "19  SuperBowl  2.923077   3.09466\n",
       "20   WildCard  4.403846  5.262742"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show it works for all seasons\n",
    "grouping_mean_sd(lst_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab059f-915e-4ad9-b45c-20409387f944",
   "metadata": {},
   "source": [
    "It is interesting to see that the away team's tend to score the most first quarter point on average in the Wild Card and Divisional Rounds (or earlier rounds in the playoffs). However, the \"home field\" advantage we are used to with the away teams struggling tend to show up more in later rounds of the playoffs. Also note that the variability (standard deviation) tends to be larger when the mean is larger meaning some observations could have made this average higher. For example, there are less playoff games played each year (and even less in later rounds) than regular season matchups. So it is not surprising to see the extreme sampling averages show up in these rounds in particular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd5c1a-19f6-429f-bbc8-379c2c69f102",
   "metadata": {},
   "source": [
    "## Part 3: Using pyspark (SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5d6f5-ebbe-4bce-bc81-76c8219e1de4",
   "metadata": {},
   "source": [
    "We'll use spark SQL functionality (rather than writing our own MapReduce type code.) To do:\n",
    "\n",
    "- Read in the full nfl data set into spark as a spark SQL style data frame\n",
    "- Use spark SQL to find the mean and standard deviation for the AQ1, AQ2, AQ3, AQ4, AFinal, HQ1, HQ2, HQ3, HQ4, and HFinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "895ac391-1270-401e-bd02-dd993afa8ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "|sum(AQ1)|sum(AQ2)|sum(AQ3)|sum(AQ4)|sum(AFinal)|sum(HQ1)|sum(HQ2)|sum(HQ3)|sum(HQ4)|sum(HFinal)|\n",
      "+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "|   13623|   21664|   15227|   20445|      71354|   16761|   24662|   16630|   21947|      80437|\n",
      "+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make data from URL into csv saved on local\n",
    "pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\").to_csv(r'NFL_data.csv',index = False, na_rep = 'N/A')\n",
    "\n",
    "# Start up Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('Warren_HW6').getOrCreate()\n",
    "\n",
    "# Load in data\n",
    "df = spark.read.load(\"NFL_data.csv\",\n",
    "                     format=\"csv\",\n",
    "                     sep=\",\",\n",
    "                     inferSchema=\"true\",\n",
    "                     header=\"true\")\n",
    "\n",
    "# Get the sum of all the data\n",
    "df.select([\"AQ1\", \"AQ2\", \"AQ3\", \"AQ4\", \"AFinal\", \"HQ1\", \"HQ2\", \"HQ3\", \"HQ4\", \"HFinal\"]).groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932540e8-3bfd-41e4-b4ff-0d7818cf7aef",
   "metadata": {},
   "source": [
    "We can see here how the 2nd and 4th quarters for away teams seem to be the best quarters, while the same holds true for the home teams. Overall, we can see how home teams perform better than away teams by scoring points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183811dc-80ff-4c83-9201-ff120e93bb5d",
   "metadata": {},
   "source": [
    "Repeat the previous item but return summaries at each level of the season variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef3259be-20ad-4e54-a392-0ea9140185e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "|season|sum(AQ1)|sum(AQ2)|sum(AQ3)|sum(AQ4)|sum(AFinal)|sum(HQ1)|sum(HQ2)|sum(HQ3)|sum(HQ4)|sum(HFinal)|\n",
      "+------+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "|  2002|    1078|    1608|    1188|    1583|       5511|    1150|    1979|    1259|    1716|       6146|\n",
      "|  2003|     950|    1627|    1045|    1430|       5103|    1342|    1769|    1218|    1681|       6055|\n",
      "|  2004|    1045|    1673|    1109|    1548|       5404|    1320|    1888|    1135|    1752|       6116|\n",
      "|  2005|    1038|    1480|    1076|    1386|       5016|    1173|    1983|    1249|    1532|       5958|\n",
      "|  2006|     969|    1650|    1149|    1619|       5408|    1230|    1613|    1219|    1584|       5676|\n",
      "|  2007|     987|    1661|    1158|    1568|       5401|    1342|    1889|    1312|    1607|       6183|\n",
      "|  2008|    1015|    1735|    1088|    1715|       5565|    1383|    1924|    1227|    1614|       6190|\n",
      "|  2009|    1033|    1656|    1148|    1587|       5442|    1265|    2080|    1129|    1572|       6082|\n",
      "|  2010|    1061|    1833|    1236|    1523|       5692|    1222|    1808|    1300|    1784|       6141|\n",
      "|  2011|    1030|    1597|    1224|    1601|       5476|    1341|    1958|    1344|    1730|       6403|\n",
      "|  2012|    1193|    1675|    1204|    1665|       5782|    1316|    1870|    1393|    1822|       6440|\n",
      "|  2013|    1080|    1781|    1338|    1651|       5868|    1365|    1988|    1404|    1851|       6646|\n",
      "|  2014|    1144|    1688|    1264|    1569|       5686|    1312|    1913|    1441|    1702|       6401|\n",
      "+------+--------+--------+--------+--------+-----------+--------+--------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the sum of all the data\n",
    "df.select([\"season\", \"AQ1\", \"AQ2\", \"AQ3\", \"AQ4\", \"AFinal\", \"HQ1\", \"HQ2\", \"HQ3\", \"HQ4\", \"HFinal\"]).groupBy(\"season\").sum().sort(\"season\").drop(\"sum(season)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4edfc-5e25-4690-a966-2515a210dab2",
   "metadata": {},
   "source": [
    "Here we can see the constant trend over the seasons that scoring is larger in the second and fourth quarters for both home and away teams. This is an interesting trend to note. It is also amazing to see how each year the home team scored more overall points than the away team. This might show home field advantage is real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3c861-7587-4167-be5e-ebe4969f0613",
   "metadata": {},
   "source": [
    "## Using pyspark (pandas-on-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687adae-ab78-4b4b-85c0-dd6664620297",
   "metadata": {},
   "source": [
    "Repeat part 3 but read the data into a pandas-on-spark data frame and use pandas-on-spark functionality\n",
    "to find the summaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc1666-d414-4eea-890a-5fb76e7231d8",
   "metadata": {},
   "source": [
    "- Read in the full nfl data set into spark as a pandas on spark style data frame\n",
    "- Use spark SQL to find the mean and standard deviation for the AQ1, AQ2, AQ3, AQ4, AFinal, HQ1, HQ2, HQ3, HQ4, and HFinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8c3af9f-b928-46dd-8c34-19d4bd1ee3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AQ1       13623\n",
       "AQ2       21664\n",
       "AQ3       15227\n",
       "AQ4       20445\n",
       "AFinal    71354\n",
       "HQ1       16761\n",
       "HQ2       24662\n",
       "HQ3       16630\n",
       "HQ4       21947\n",
       "HFinal    80437\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pyspark on pandas\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Read in the data\n",
    "pdf = ps.read_csv(\"NFL_data.csv\")\n",
    "\n",
    "# Make the first sum without grouping\n",
    "pdf[[\"AQ1\", \"AQ2\", \"AQ3\", \"AQ4\", \"AFinal\", \"HQ1\", \"HQ2\", \"HQ3\", \"HQ4\", \"HFinal\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4760054-d334-4e47-8e06-667d8eb69514",
   "metadata": {},
   "source": [
    "Same trends as before from what we saw in **Part 3**. Repeat the previous item but return summaries at each level of the season variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cbc11c10-4861-487c-96bb-345fc258895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/groupby.py:893: FutureWarning: Default value of `numeric_only` will be changed to `False` instead of `True` in 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>AQ1</th>\n",
       "      <th>AQ2</th>\n",
       "      <th>AQ3</th>\n",
       "      <th>AQ4</th>\n",
       "      <th>AFinal</th>\n",
       "      <th>HQ1</th>\n",
       "      <th>HQ2</th>\n",
       "      <th>HQ3</th>\n",
       "      <th>HQ4</th>\n",
       "      <th>HFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2002</td>\n",
       "      <td>1078</td>\n",
       "      <td>1608</td>\n",
       "      <td>1188</td>\n",
       "      <td>1583</td>\n",
       "      <td>5511</td>\n",
       "      <td>1150</td>\n",
       "      <td>1979</td>\n",
       "      <td>1259</td>\n",
       "      <td>1716</td>\n",
       "      <td>6146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>950</td>\n",
       "      <td>1627</td>\n",
       "      <td>1045</td>\n",
       "      <td>1430</td>\n",
       "      <td>5103</td>\n",
       "      <td>1342</td>\n",
       "      <td>1769</td>\n",
       "      <td>1218</td>\n",
       "      <td>1681</td>\n",
       "      <td>6055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004</td>\n",
       "      <td>1045</td>\n",
       "      <td>1673</td>\n",
       "      <td>1109</td>\n",
       "      <td>1548</td>\n",
       "      <td>5404</td>\n",
       "      <td>1320</td>\n",
       "      <td>1888</td>\n",
       "      <td>1135</td>\n",
       "      <td>1752</td>\n",
       "      <td>6116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2005</td>\n",
       "      <td>1038</td>\n",
       "      <td>1480</td>\n",
       "      <td>1076</td>\n",
       "      <td>1386</td>\n",
       "      <td>5016</td>\n",
       "      <td>1173</td>\n",
       "      <td>1983</td>\n",
       "      <td>1249</td>\n",
       "      <td>1532</td>\n",
       "      <td>5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>969</td>\n",
       "      <td>1650</td>\n",
       "      <td>1149</td>\n",
       "      <td>1619</td>\n",
       "      <td>5408</td>\n",
       "      <td>1230</td>\n",
       "      <td>1613</td>\n",
       "      <td>1219</td>\n",
       "      <td>1584</td>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>987</td>\n",
       "      <td>1661</td>\n",
       "      <td>1158</td>\n",
       "      <td>1568</td>\n",
       "      <td>5401</td>\n",
       "      <td>1342</td>\n",
       "      <td>1889</td>\n",
       "      <td>1312</td>\n",
       "      <td>1607</td>\n",
       "      <td>6183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2008</td>\n",
       "      <td>1015</td>\n",
       "      <td>1735</td>\n",
       "      <td>1088</td>\n",
       "      <td>1715</td>\n",
       "      <td>5565</td>\n",
       "      <td>1383</td>\n",
       "      <td>1924</td>\n",
       "      <td>1227</td>\n",
       "      <td>1614</td>\n",
       "      <td>6190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009</td>\n",
       "      <td>1033</td>\n",
       "      <td>1656</td>\n",
       "      <td>1148</td>\n",
       "      <td>1587</td>\n",
       "      <td>5442</td>\n",
       "      <td>1265</td>\n",
       "      <td>2080</td>\n",
       "      <td>1129</td>\n",
       "      <td>1572</td>\n",
       "      <td>6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010</td>\n",
       "      <td>1061</td>\n",
       "      <td>1833</td>\n",
       "      <td>1236</td>\n",
       "      <td>1523</td>\n",
       "      <td>5692</td>\n",
       "      <td>1222</td>\n",
       "      <td>1808</td>\n",
       "      <td>1300</td>\n",
       "      <td>1784</td>\n",
       "      <td>6141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2011</td>\n",
       "      <td>1030</td>\n",
       "      <td>1597</td>\n",
       "      <td>1224</td>\n",
       "      <td>1601</td>\n",
       "      <td>5476</td>\n",
       "      <td>1341</td>\n",
       "      <td>1958</td>\n",
       "      <td>1344</td>\n",
       "      <td>1730</td>\n",
       "      <td>6403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012</td>\n",
       "      <td>1193</td>\n",
       "      <td>1675</td>\n",
       "      <td>1204</td>\n",
       "      <td>1665</td>\n",
       "      <td>5782</td>\n",
       "      <td>1316</td>\n",
       "      <td>1870</td>\n",
       "      <td>1393</td>\n",
       "      <td>1822</td>\n",
       "      <td>6440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>1080</td>\n",
       "      <td>1781</td>\n",
       "      <td>1338</td>\n",
       "      <td>1651</td>\n",
       "      <td>5868</td>\n",
       "      <td>1365</td>\n",
       "      <td>1988</td>\n",
       "      <td>1404</td>\n",
       "      <td>1851</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>1144</td>\n",
       "      <td>1688</td>\n",
       "      <td>1264</td>\n",
       "      <td>1569</td>\n",
       "      <td>5686</td>\n",
       "      <td>1312</td>\n",
       "      <td>1913</td>\n",
       "      <td>1441</td>\n",
       "      <td>1702</td>\n",
       "      <td>6401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    season   AQ1   AQ2   AQ3   AQ4  AFinal   HQ1   HQ2   HQ3   HQ4  HFinal\n",
       "12    2002  1078  1608  1188  1583    5511  1150  1979  1259  1716    6146\n",
       "0     2003   950  1627  1045  1430    5103  1342  1769  1218  1681    6055\n",
       "5     2004  1045  1673  1109  1548    5404  1320  1888  1135  1752    6116\n",
       "8     2005  1038  1480  1076  1386    5016  1173  1983  1249  1532    5958\n",
       "2     2006   969  1650  1149  1619    5408  1230  1613  1219  1584    5676\n",
       "1     2007   987  1661  1158  1568    5401  1342  1889  1312  1607    6183\n",
       "11    2008  1015  1735  1088  1715    5565  1383  1924  1227  1614    6190\n",
       "7     2009  1033  1656  1148  1587    5442  1265  2080  1129  1572    6082\n",
       "9     2010  1061  1833  1236  1523    5692  1222  1808  1300  1784    6141\n",
       "10    2011  1030  1597  1224  1601    5476  1341  1958  1344  1730    6403\n",
       "6     2012  1193  1675  1204  1665    5782  1316  1870  1393  1822    6440\n",
       "3     2013  1080  1781  1338  1651    5868  1365  1988  1404  1851    6646\n",
       "4     2014  1144  1688  1264  1569    5686  1312  1913  1441  1702    6401"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the sum with grouping on season\n",
    "pdf[[\"season\", \"AQ1\", \"AQ2\", \"AQ3\", \"AQ4\", \"AFinal\", \"HQ1\", \"HQ2\", \"HQ3\", \"HQ4\", \"HFinal\"]].groupby(\"season\", as_index = False).sum().sort_values(by = ['season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50124c40-828d-47aa-a55c-16d13d66182e",
   "metadata": {},
   "source": [
    "Again we get the same trends that are mentioned in **Part 3**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
